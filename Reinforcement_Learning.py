{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# 1. Modelagem do ambiente 2D\n",
        "class Environment:\n",
        "    def __init__(self, grid_size, obstacles, delivery_points): # Inicializa o ambiente com o tamanho da grade, obstáculos e pontos de entrega.\n",
        "        self.grid_size = grid_size\n",
        "        self.obstacles = obstacles\n",
        "        self.delivery_points = delivery_points\n",
        "        self.agent_position = (0, 0) # Definir a posição inicial do agente.\n",
        "\n",
        "    def is_valid_move(self, action): # Verificar se o movimento é válido, considerando limites da grade e obstáculos.\n",
        "        x, y = self.agent_position\n",
        "        if action == 'up':\n",
        "            x -= 1\n",
        "        elif action == 'down':\n",
        "            x += 1\n",
        "        elif action == 'left':\n",
        "            y -= 1\n",
        "        elif action == 'right':\n",
        "            y += 1\n",
        "\n",
        "        if x < 0 or x >= self.grid_size[0] or y < 0 or y >= self.grid_size[1] or (x, y) in self.obstacles:\n",
        "            return False\n",
        "        return True\n",
        "\n",
        "    def take_action(self, action): # Realizar o movimento, caso seja válido.\n",
        "        if self.is_valid_move(action):\n",
        "            x, y = self.agent_position\n",
        "            if action == 'up':\n",
        "                x -= 1\n",
        "            elif action == 'down':\n",
        "                x += 1\n",
        "            elif action == 'left':\n",
        "                y -= 1\n",
        "            elif action == 'right':\n",
        "                y += 1\n",
        "            self.agent_position = (x, y)\n",
        "\n",
        "    def is_at_delivery_point(self): # Verificar se o agente está em um ponto de entrega.\n",
        "        return self.agent_position in self.delivery_points\n",
        "\n",
        "# 2. Definição do MDP\n",
        "class MDP:\n",
        "    def __init__(self, environment, discount_factor, exploration_prob):\n",
        "       # Inicializar o MDP com o ambiente, fator de desconto e probabilidade de exploração.\n",
        "       self.environment = environment #ambiente\n",
        "       self.discount_factor = discount_factor #fator de desconto\n",
        "       self.exploration_prob = exploration_prob #probabilidade de exploração\n",
        "\n",
        "    def get_possible_actions(self): # Retornar as possíveis ações que o agente pode realizar.\n",
        "        return ['up', 'down', 'left', 'right']\n",
        "\n",
        "# 3. Implementação da Q-Table\n",
        "class QTable:\n",
        "    def __init__(self, state_space, action_space):  # Inicializar a tabela Q com zeros.\n",
        "        self.q_table = np.zeros((state_space, action_space))\n",
        "\n",
        "    def get_value(self, state, action):  # Obter o valor da tabela Q para um estado e ação específicos.\n",
        "        return self.q_table[state, action]\n",
        "\n",
        "    def set_value(self, state, action, value): # Definir o valor da tabela Q para um estado e ação específicos.\n",
        "        self.q_table[state, action] = value\n",
        "\n",
        "# 4. Implementação do agente\n",
        "class QLearningAgent:\n",
        "    def __init__(self, environment, mdp, q_table, learning_rate, exploration_prob):\n",
        "      # Inicializar o agente de Q-Learning com o ambiente, MDP, tabela Q, taxa de aprendizado e probabilidade de exploração.\n",
        "        self.environment = environment\n",
        "        self.mdp = mdp\n",
        "        self.q_table = q_table\n",
        "        self.learning_rate = learning_rate\n",
        "        self.exploration_prob = exploration_prob\n",
        "\n",
        "    def select_action(self):    # Escolher uma ação com base na probabilidade de exploração ou nos valores da tabela Q.\n",
        "        if random.uniform(0, 1) < self.exploration_prob:\n",
        "            return random.choice(self.mdp.get_possible_actions())\n",
        "        else:\n",
        "            state = self.environment.agent_position\n",
        "            possible_actions = self.mdp.get_possible_actions()\n",
        "            q_values = [self.q_table.get_value(state, possible_actions.index(action)) for action in possible_actions]\n",
        "            return possible_actions[np.argmax(q_values)]\n",
        "\n",
        "    def update_q_table(self, state, action, reward, next_state):  # Atualizar a tabela Q com base na recompensa obtida.\n",
        "        max_q = np.max([self.q_table.get_value(next_state, self.mdp.get_possible_actions().index(a)) for a in self.mdp.get_possible_actions()])\n",
        "        current_q = self.q_table.get_value(state, self.mdp.get_possible_actions().index(action))\n",
        "        new_q = (1 - self.learning_rate) * current_q + self.learning_rate * (reward + self.mdp.discount_factor * max_q)\n",
        "        self.q_table.set_value(state, self.mdp.get_possible_actions().index(action), new_q)\n",
        "\n",
        "# 5. Living Penalty\n",
        "def apply_living_penalty(reward, time_penalty):   # Aplicar uma penalidade por à recompensa, baseada no custo de tempo\n",
        "    return reward - time_penalty\n"
      ],
      "metadata": {
        "id": "U-4IEOL1INPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. treinamento e avaliação do agente\n",
        "def train_and_evaluate(grid_size, obstacles, delivery_points, discount_factor, exploration_prob, learning_rate, num_episodes, time_penalty):\n",
        "    test_environment = Environment(grid_size, obstacles, delivery_points)  # Cria um ambiente de teste com o tamanho da grade, obstáculos e pontos de entrega fornecidos\n",
        "\n",
        "    test_mdp = MDP(test_environment, discount_factor, exploration_prob) # Cria um MDP de teste com base no ambiente, fator de desconto e probabilidade de exploração\n",
        "    state_space = np.prod(grid_size) # Calcula o espaço de estados multiplicando as dimensões da grade\n",
        "    action_space = len(test_mdp.get_possible_actions()) # Obtém o número de ações possíveis no MDP\n",
        "    test_q_table = QTable(state_space, action_space) # Cria uma tabela Q com base no espaço de estados e ações possíveis\n",
        "\n",
        "    # Cria um agente Q-Learning com o ambiente, MDP, tabela Q, taxa de aprendizado e probabilidade de exploração\n",
        "    test_agent = QLearningAgent(test_environment, test_mdp, test_q_table, learning_rate, exploration_prob)\n",
        "\n",
        "    total_rewards = [] # Inicializa uma lista para armazenar as recompensas totais de cada episódio\n",
        "\n",
        "    # Loop através de vários episódios de treinamento e avaliação\n",
        "    for episode in range(num_episodes):  # Inicializa o estado do agente no ambiente\n",
        "\n",
        "        state = test_environment.agent_position\n",
        "        # Inicializa a recompensa total para o episódio atual\n",
        "        total_reward = 0\n",
        "\n",
        "        while not test_environment.is_at_delivery_point(): # Loop até que o agente alcance um ponto de entrega\n",
        "            possible_actions = test_mdp.get_possible_actions() # Obtém a lista de ações possíveis do MDP\n",
        "            action = test_agent.select_action() # O agente escolhe uma ação com base em sua estratégia (Q-Learning)\n",
        "\n",
        "            if action not in possible_actions:  # Garante que a ação escolhida seja uma das ações possíveis\n",
        "                action = random.choice(possible_actions)\n",
        "\n",
        "            test_environment.take_action(action)  # O agente realiza a ação no ambiente e obtém uma recompensa\n",
        "            reward = -0.1 # Define uma recompensa padrão de -0.1 por movimento\n",
        "\n",
        "            if test_environment.is_at_delivery_point():  # Se o agente alcançar um ponto de entrega, a recompensa é definida como 1.0 (recompensa máxima)\n",
        "                reward = 1.0\n",
        "\n",
        "            reward = apply_living_penalty(reward, time_penalty) # Aplica uma penalidade de tempo à recompensa\n",
        "            total_reward += reward # Atualiza a recompensa total do episódio\n",
        "            next_state = test_environment.agent_position  # Obtém o próximo estado após a ação\n",
        "            test_agent.update_q_table(state, action, reward, next_state) # Atualiza a tabela Q do agente com base na recompensa obtida\n",
        "            state = next_state # Atualiza o estado atual para o próximo estado\n",
        "\n",
        "        total_rewards.append(total_reward) # Armazena a recompensa total do episódio atual na lista de recompensas\n",
        "    average_reward = np.mean(total_rewards) # Calcula a recompensa média ao longo de todos os episódios\n",
        "    return average_reward # Retorna a recompensa média como resultado\n",
        "\n",
        "# Parâmetros de teste\n",
        "grid_size = (4, 4)\n",
        "obstacles = [(1, 1), (2, 2)]\n",
        "delivery_points = [(3, 3), (0, 2)]\n",
        "discount_factor = 0.9\n",
        "exploration_prob = 0.2\n",
        "learning_rate = 0.1\n",
        "num_episodes = 1000\n",
        "time_penalty = 0.1\n",
        "\n",
        "# Chama a função de treinamento e avaliação com os parâmetros de teste\n",
        "average_reward = train_and_evaluate(grid_size, obstacles, delivery_points, discount_factor, exploration_prob, learning_rate, num_episodes, time_penalty)\n",
        "\n",
        "# Imprime a recompensa média obtida após o treinamento\n",
        "print(\"Recompensa média: \", average_reward)\n"
      ],
      "metadata": {
        "id": "rjzKe4WgvKtq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}