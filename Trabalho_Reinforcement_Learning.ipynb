{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/biancafsena/REINFORCEMENT-LEARNING/blob/main/Trabalho_Reinforcement_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Bibliotecas**"
      ],
      "metadata": {
        "id": "4uyzv-WU-m1Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random"
      ],
      "metadata": {
        "id": "zKpfAUsC-qTX"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Modelagem do ambiente 2D**"
      ],
      "metadata": {
        "id": "W0TKopLP9nHb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Environment:\n",
        "    def __init__(self, grid_size, obstacles, delivery_points): # Inicializa o ambiente com o tamanho da grade, obstáculos e pontos de entrega.\n",
        "        self.grid_size = grid_size\n",
        "        self.obstacles = obstacles\n",
        "        self.delivery_points = delivery_points\n",
        "        self.agent_position = (0, 0) # Definir a posição inicial do agente.\n",
        "\n",
        "    def is_valid_move(self, action): # Verificar se o movimento é válido, considerando limites da grade e obstáculos.\n",
        "        x, y = self.agent_position\n",
        "        if action == 'up':\n",
        "            x -= 1\n",
        "        elif action == 'down':\n",
        "            x += 1\n",
        "        elif action == 'left':\n",
        "            y -= 1\n",
        "        elif action == 'right':\n",
        "            y += 1\n",
        "\n",
        "        if x < 0 or x >= self.grid_size[0] or y < 0 or y >= self.grid_size[1] or (x, y) in self.obstacles:\n",
        "            return False\n",
        "        return True"
      ],
      "metadata": {
        "id": "tRM3QYwh9fHr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "    def take_action(self, action): # Realizar o movimento, caso seja válido.\n",
        "        if self.is_valid_move(action):\n",
        "            x, y = self.agent_position\n",
        "            if action == 'up':\n",
        "                x -= 1\n",
        "            elif action == 'down':\n",
        "                x += 1\n",
        "            elif action == 'left':\n",
        "                y -= 1\n",
        "            elif action == 'right':\n",
        "                y += 1\n",
        "            self.agent_position = (x, y)\n",
        "\n",
        "    def is_at_delivery_point(self): # Verificar se o agente está em um ponto de entrega.\n",
        "        return self.agent_position in self.delivery_points"
      ],
      "metadata": {
        "id": "ZfAMNMac9suw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Definição do MDP**"
      ],
      "metadata": {
        "id": "R1lMHFXX845V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MDP:\n",
        "    def __init__(self, environment, discount_factor, exploration_prob):\n",
        "       # Inicializar o MDP com o ambiente, fator de desconto e probabilidade de exploração.\n",
        "       self.environment = environment #ambiente\n",
        "       self.discount_factor = discount_factor #fator de desconto\n",
        "       self.exploration_prob = exploration_prob #probabilidade de exploração\n",
        "\n",
        "    def get_possible_actions(self): # Retornar as possíveis ações que o agente pode realizar.\n",
        "        return ['up', 'down', 'left', 'right']"
      ],
      "metadata": {
        "id": "bFzEtT8v98Hr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. Implementação da Q-Table**"
      ],
      "metadata": {
        "id": "hXOscSIS-BDA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QTable:\n",
        "    def __init__(self, state_space, action_space):  # Inicializar a tabela Q com zeros.\n",
        "        self.q_table = np.zeros((state_space, action_space))\n",
        "\n",
        "    def get_value(self, state, action):  # Obter o valor da tabela Q para um estado e ação específicos.\n",
        "        return self.q_table[state, action]\n",
        "\n",
        "    def set_value(self, state, action, value): # Definir o valor da tabela Q para um estado e ação específicos.\n",
        "        self.q_table[state, action] = value"
      ],
      "metadata": {
        "id": "Q_my40UO98OZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4. Implementação do agente**"
      ],
      "metadata": {
        "id": "7Wh0ZzsH-L_x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QLearningAgent:\n",
        "    def __init__(self, environment, mdp, q_table, learning_rate, exploration_prob):\n",
        "      # Inicializar o agente de Q-Learning com o ambiente, MDP, tabela Q, taxa de aprendizado e probabilidade de exploração.\n",
        "        self.environment = environment\n",
        "        self.mdp = mdp\n",
        "        self.q_table = q_table\n",
        "        self.learning_rate = learning_rate\n",
        "        self.exploration_prob = exploration_prob\n",
        "\n",
        "    def select_action(self):    # Escolher uma ação com base na probabilidade de exploração ou nos valores da tabela Q.\n",
        "        if random.uniform(0, 1) < self.exploration_prob:\n",
        "            return random.choice(self.mdp.get_possible_actions())\n",
        "        else:\n",
        "            state = self.environment.agent_position\n",
        "            possible_actions = self.mdp.get_possible_actions()\n",
        "            q_values = [self.q_table.get_value(state, possible_actions.index(action)) for action in possible_actions]\n",
        "            return possible_actions[np.argmax(q_values)]\n",
        "\n",
        "    def update_q_table(self, state, action, reward, next_state):  # Atualizar a tabela Q com base na recompensa obtida.\n",
        "        max_q = np.max([self.q_table.get_value(next_state, self.mdp.get_possible_actions().index(a)) for a in self.mdp.get_possible_actions()])\n",
        "        current_q = self.q_table.get_value(state, self.mdp.get_possible_actions().index(action))\n",
        "        new_q = (1 - self.learning_rate) * current_q + self.learning_rate * (reward + self.mdp.discount_factor * max_q)\n",
        "        self.q_table.set_value(state, self.mdp.get_possible_actions().index(action), new_q)"
      ],
      "metadata": {
        "id": "0yakqxCI98TI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5. Living Penalty**"
      ],
      "metadata": {
        "id": "VLTSpNh2-XuH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_living_penalty(reward, time_penalty):   # Aplicar uma penalidade por à recompensa, baseada no custo de tempo\n",
        "    return reward - time_penalty"
      ],
      "metadata": {
        "id": "aczonRrw98YT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6. Treinamento e Avaliação do agente**"
      ],
      "metadata": {
        "id": "QRicHMF2CwbH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate(grid_size, obstacles, delivery_points, discount_factor, exploration_prob, learning_rate, num_episodes, time_penalty):\n",
        "    test_environment = Environment(grid_size, obstacles, delivery_points)  # Cria um ambiente de teste com o tamanho da grade, obstáculos e pontos de entrega fornecidos\n",
        "\n",
        "class Environment:\n",
        "    def __init__(self, grid_size, obstacles, delivery_points):\n",
        "        self.grid_size = grid_size\n",
        "        self.obstacles = obstacles\n",
        "        self.delivery_points = delivery_points\n",
        "        self.agent_position = (0, 0)\n",
        "\n",
        "    def is_valid_move(self, action):\n",
        "        x, y = self.agent_position\n",
        "        if action == 'up':\n",
        "            x -= 1\n",
        "        elif action == 'down':\n",
        "            x += 1\n",
        "        elif action == 'left':\n",
        "            y -= 1\n",
        "        elif action == 'right':\n",
        "            y += 1\n",
        "\n",
        "        if x < 0 or x >= self.grid_size[0] or y < 0 or y >= self.grid_size[1] or (x, y) in self.obstacles:\n",
        "            return False\n",
        "        return True\n",
        "\n",
        "    def take_action(self, action):\n",
        "        if self.is_valid_move(action):\n",
        "            x, y = self.agent_position\n",
        "            if action == 'up':\n",
        "                x -= 1\n",
        "            elif action == 'down':\n",
        "                x += 1\n",
        "            elif action == 'left':\n",
        "                y -= 1\n",
        "            elif action == 'right':\n",
        "                y += 1\n",
        "            self.agent_position = (x, y)\n",
        "\n",
        "    def is_at_delivery_point(self):\n",
        "        return self.agent_position in self.delivery_points\n",
        "\n",
        "\n",
        "class MDP:\n",
        "    def __init__(self, environment, discount_factor, exploration_prob):\n",
        "        self.environment = environment\n",
        "        self.discount_factor = discount_factor\n",
        "        self.exploration_prob = exploration_prob\n",
        "\n",
        "    def get_possible_actions(self):\n",
        "        return ['up', 'down', 'left', 'right']\n",
        "\n",
        "class QTable:\n",
        "    def __init__(self, state_space, action_space):\n",
        "        self.q_table = np.zeros((state_space, action_space))\n",
        "\n",
        "    def get_value(self, state, action):\n",
        "        return self.q_table[state, action]\n",
        "\n",
        "    def set_value(self, state, action, value):\n",
        "        self.q_table[state, action] = value\n",
        "\n",
        "class QLearningAgent:\n",
        "    def __init__(self, environment, mdp, q_table, learning_rate, exploration_prob):\n",
        "        self.environment = environment\n",
        "        self.mdp = mdp\n",
        "        self.q_table = q_table\n",
        "        self.learning_rate = learning_rate\n",
        "        self.exploration_prob = exploration_prob\n",
        "\n",
        "    def select_action(self):\n",
        "        if random.uniform(0, 1) < self.exploration_prob:\n",
        "            return random.choice(self.mdp.get_possible_actions())\n",
        "        else:\n",
        "            state = self.environment.agent_position\n",
        "            possible_actions = self.mdp.get_possible_actions()\n",
        "            q_values = [self.q_table.get_value(state, possible_actions.index(action)) for action in possible_actions]\n",
        "            max_q_index = np.argmax(q_values)\n",
        "            if max_q_index < len(possible_actions):\n",
        "                return possible_actions[max_q_index]\n",
        "            else:\n",
        "                return random.choice(possible_actions)\n",
        "\n",
        "    def update_q_table(self, state, action, reward, next_state):\n",
        "        max_q = np.max([self.q_table.get_value(next_state, self.mdp.get_possible_actions().index(a)) for a in self.mdp.get_possible_actions()])\n",
        "        current_q = self.q_table.get_value(state, self.mdp.get_possible_actions().index(action))\n",
        "        new_q = (1 - self.learning_rate) * current_q + self.learning_rate * (reward + self.mdp.discount_factor * max_q)\n",
        "        self.q_table.set_value(state, self.mdp.get_possible_actions().index(action), new_q)\n",
        "\n",
        "def apply_living_penalty(reward, time_penalty):\n",
        "    return reward - time_penalty\n",
        "\n",
        "def train_and_evaluate(grid_size, obstacles, delivery_points, discount_factor, exploration_prob, learning_rate, num_episodes, time_penalty):\n",
        "    test_environment = Environment(grid_size, obstacles, delivery_points)\n",
        "    test_mdp = MDP(test_environment, discount_factor, exploration_prob)\n",
        "    state_space = np.prod(grid_size)\n",
        "    action_space = len(test_mdp.get_possible_actions())\n",
        "    test_q_table = QTable(state_space, action_space)\n",
        "    test_agent = QLearningAgent(test_environment, test_mdp, test_q_table, learning_rate, exploration_prob)\n",
        "    total_rewards = []\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state = test_environment.agent_position\n",
        "        total_reward = 0\n",
        "\n",
        "        state = test_environment.agent_position\n",
        "        # Inicializa a recompensa total para o episódio atual\n",
        "        total_reward = 0\n",
        "\n",
        "        while not test_environment.is_at_delivery_point(): # Loop até que o agente alcance um ponto de entrega\n",
        "            possible_actions = test_mdp.get_possible_actions() # Obtém a lista de ações possíveis do MDP\n",
        "            action = test_agent.select_action() # O agente escolhe uma ação com base em sua estratégia (Q-Learning)\n",
        "\n",
        "            if action not in possible_actions:  # Garante que a ação escolhida seja uma das ações possíveis\n",
        "                action = random.choice(possible_actions)\n",
        "\n",
        "                test_environment.take_action(action)  # O agente realiza a ação no ambiente e obtém uma recompensa\n",
        "            reward = -0.1 # Define uma recompensa padrão de -0.1 por movimento\n",
        "\n",
        "            if test_environment.is_at_delivery_point():  # Se o agente alcançar um ponto de entrega, a recompensa é definida como 1.0 (recompensa máxima)\n",
        "                reward = 1.0\n",
        "\n",
        "                reward = apply_living_penalty(reward, time_penalty) # Aplica uma penalidade de tempo à recompensa\n",
        "            total_reward += reward # Atualiza a recompensa total do episódio\n",
        "            next_state = test_environment.agent_position  # Obtém o próximo estado após a ação\n",
        "            test_agent.update_q_table(state, action, reward, next_state) # Atualiza a tabela Q do agente com base na recompensa obtida\n",
        "            state = next_state # Atualiza o estado atual para o próximo estado\n",
        "\n",
        "        total_rewards.append(total_reward) # Armazena a recompensa total do episódio atual na lista de recompensas\n",
        "    average_reward = np.mean(total_rewards) # Calcula a recompensa média ao longo de todos os episódios\n",
        "    return average_reward # Retorna a recompensa média como resultado\n",
        "\n",
        "        # Parâmetros de teste\n",
        "grid_size = (4, 4)\n",
        "obstacles = [(1, 1), (2, 2)]\n",
        "delivery_points = [(3, 3), (0, 2)]\n",
        "discount_factor = 0.9\n",
        "exploration_prob = 0.2\n",
        "learning_rate = 0.1\n",
        "num_episodes = 1000\n",
        "time_penalty = 0.1\n",
        "\n",
        "# Chama a função de treinamento e avaliação com os parâmetros de teste\n",
        "average_reward = train_and_evaluate(grid_size, obstacles, delivery_points, discount_factor, exploration_prob, learning_rate, num_episodes, time_penalty)\n",
        "\n",
        "# Imprime a recompensa média obtida após o treinamento\n",
        "print(\"Recompensa média: \", average_reward)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "jQLsTS2WC9FD",
        "outputId": "5bf09260-01db-4de7-f12e-3057465406e3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-b73e3ad0b82a>\u001b[0m in \u001b[0;36m<cell line: 143>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;31m# Chama a função de treinamento e avaliação com os parâmetros de teste\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m \u001b[0maverage_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobstacles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelivery_points\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscount_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexploration_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_penalty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;31m# Imprime a recompensa média obtida após o treinamento\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-b73e3ad0b82a>\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(grid_size, obstacles, delivery_points, discount_factor, exploration_prob, learning_rate, num_episodes, time_penalty)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0mtest_environment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEnvironment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobstacles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelivery_points\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mtest_mdp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMDP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_environment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscount_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexploration_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0mstate_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m     \u001b[0maction_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_mdp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_possible_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0mtest_q_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_space\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ]
    }
  ]
}